{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: data preprocessing\n",
    "# step 2: build model\n",
    "# step 3: train model\n",
    "# step 4: apply model - generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "with open('/Users/pliu/Downloads/anna.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# get vocabulary\n",
    "vocab = set(text)\n",
    "\n",
    "# vocab-int mapping dict\n",
    "vocab_to_int = {c:i for i, c in enumerate(vocab)}\n",
    "\n",
    "# int-vocab mapping dict\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "\n",
    "# encode text\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy fa'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset via mini batch\n",
    "# each batch is a N*M array, batch_size = n_seqs*n_steps\n",
    "# n_seqs with len of N, how many samples in each batch\n",
    "# n_steps iwth len of M, n_steps, \n",
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    \"\"\"\n",
    "    split array (original data) into batches (mini-batch)\n",
    "    args:\n",
    "        arr: the array to split\n",
    "        n_seqs: num of sequence in a batch\n",
    "        n_steps: length of each sequence\n",
    "    \"\"\"\n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = int(len(arr) / batch_size)\n",
    "    \n",
    "    arr = arr[:batch_size * n_batches]\n",
    "    \n",
    "    arr = arr.reshape((n_seqs,-1))\n",
    "    \n",
    "    # generator\n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # inputs\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # targets\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, n_seqs=10, n_steps=50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[57 22 60 51  1 68 64 66 18 48]\n",
      " [66 60 14 66 24 80  1 66 21 80]\n",
      " [67 26 24 30 48 48 39 40 68 50]\n",
      " [24 66 73 12 64 26 24 21 66 22]\n",
      " [66 26  1 66 26 50 65 66 50 26]\n",
      " [66  5  1 66 20 60 50 48 80 24]\n",
      " [22 68 24 66 62 80 14 68 66 56]\n",
      " [43 66 15 12  1 66 24 80 20 66]\n",
      " [ 1 66 26 50 24 59  1 30 66 27]\n",
      " [66 50 60 26 73 66  1 80 66 22]]\n",
      "\n",
      "y\n",
      " [[22 60 51  1 68 64 66 18 48 48]\n",
      " [60 14 66 24 80  1 66 21 80 26]\n",
      " [26 24 30 48 48 39 40 68 50 65]\n",
      " [66 73 12 64 26 24 21 66 22 26]\n",
      " [26  1 66 26 50 65 66 50 26 64]\n",
      " [ 5  1 66 20 60 50 48 80 24 17]\n",
      " [68 24 66 62 80 14 68 66 56 80]\n",
      " [66 15 12  1 66 24 80 20 66 50]\n",
      " [66 26 50 24 59  1 30 66 27 22]\n",
      " [50 60 26 73 66  1 80 66 22 68]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10,:10])\n",
    "print('\\ny\\n', y[:10,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Layers: input, lstm, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer\n",
    "# size of input layer depends on the batch size\n",
    "def build_inputs(n_seqs, n_steps):\n",
    "    \"\"\"\n",
    "    build input layer\n",
    "    args:\n",
    "        n_seqs: num of sequence in each batch\n",
    "        n_steps: length of char in each sequence\n",
    "    \"\"\"\n",
    "    inputs = tf.placeholder(tf.int32, shape=(n_seqs, n_steps), name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, shape=(n_seqs, n_steps), name='targets')\n",
    "    \n",
    "    # keep probability, for dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    \"\"\"\n",
    "    build lstm layer\n",
    "    args:\n",
    "        lstm_size: number of nodes in hidden layer\n",
    "        num_layers: number of lstm layers\n",
    "        batch_size: n_seq * n_steps\n",
    "        keep_prob: parameter for dropout\n",
    "    \"\"\"\n",
    "    # build a lstm cell\n",
    "    def get_lstm_cell():\n",
    "        lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "        # add dropout regularization\n",
    "        return tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)  \n",
    "    \n",
    "    # add mutiple lstm cell\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([get_lstm_cell() for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_out(lstm_output, in_size, out_size):\n",
    "    \"\"\"\n",
    "    build output layer\n",
    "    args:\n",
    "        lstm_output: output of lstm layer, a 3D array\n",
    "        in_size: size of reshaped lstm\n",
    "        out_size: size of softmax\n",
    "    return:\n",
    "        logits\n",
    "        probability distribution after softmax\n",
    "    \"\"\"\n",
    "    # reshape lstm output\n",
    "    # concat output of lstm by column, 2D => 1D\n",
    "    # [[1,2,3], [4,5,6]] => [1,2,3,4,5,6]\n",
    "    seq_output = tf.concat(1, lstm_output)\n",
    "    # reshape\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    \n",
    "    # connect lstm to softmax layer\n",
    "    with tf.variable_score('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal([in_size, out_size],stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "        \n",
    "    # calculate logits\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    \n",
    "    # get probability distribution from softmax layer\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lsmt_size, num_classes):\n",
    "    \"\"\"\n",
    "    calculate training loss based on logits and targets\n",
    "    args:\n",
    "        logits: output of fully-connected layer, without softmax\n",
    "        targets\n",
    "        lsmt_size: number of nodes in lsmt layer\n",
    "        num_classes: vocab_size, number of classes\n",
    "    return:\n",
    "        loss, softmax reduce entropy with logits\n",
    "    \"\"\"\n",
    "    # one-hot encoding to targets\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # soft corss entropy between logits and labels\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tr.reduce_mean(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "# RNN#s problems: gradients exploding and gradients disappering\n",
    "# LSTM solves the problem of gradient disapppering\n",
    "# use gradient clipping to deal with gradient exploding\n",
    "# gradient clipping limits value of a gradient to a threshold\n",
    "\n",
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    \"\"\"\n",
    "    build optimizer \n",
    "    loss: loss\n",
    "    learning_rate: learning rate\n",
    "    \"\"\"\n",
    "    # use gradient clipping\n",
    "    t_vars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, t_vars), grad_clip)\n",
    "    # Adam optimizer\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50,\n",
    "                 lstm_size=128, num_layers=2, learning_rate=0.001,\n",
    "                 grad_clips=5, sampling=False):\n",
    "        # if sampling ture, use SGD\n",
    "        if sampling:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # input layer\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "        \n",
    "        # lstm layer\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "        \n",
    "        # encode inputs: one hot encoding\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # run RNN, use tf.nn.dynamic_rnn\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # predict\n",
    "        self.prediction, self.logits = build_out(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # loss and optimizer\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clips)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tensor conversion requested dtype int32 for Tensor with dtype float32: 'Tensor(\"rnn/transpose_1:0\", shape=(100, 100, 512), dtype=float32)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-87b278b61db4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n\u001b[1;32m      4\u001b[0m                 \u001b[0mlstm_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                 learning_rate=learning_rate)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-18a3947f9dc6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_classes, batch_size, num_steps, lstm_size, num_layers, learning_rate, grad_clips, sampling)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# loss and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-9f004779ed85>\u001b[0m in \u001b[0;36mbuild_out\u001b[0;34m(lstm_output, in_size, out_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# concat output of lstm by column, 2D => 1D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# [[1,2,3], [4,5,6]] => [1,2,3,4,5,6]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mseq_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;31m# reshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/envs/ml36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1109\u001b[0m       ops.convert_to_tensor(\n\u001b[1;32m   1110\u001b[0m           \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"concat_dim\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m           \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m               tensor_shape.scalar())\n\u001b[1;32m   1113\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/envs/ml36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m    996\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/envs/ml36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/envs/ml36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_TensorTensorConversionFunction\u001b[0;34m(t, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    929\u001b[0m     raise ValueError(\n\u001b[1;32m    930\u001b[0m         \u001b[0;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m         (dtype.name, t.dtype.name, str(t)))\n\u001b[0m\u001b[1;32m    932\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor conversion requested dtype int32 for Tensor with dtype float32: 'Tensor(\"rnn/transpose_1:0\", shape=(100, 100, 512), dtype=float32)'"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "save_every_n = 200 # num of rounds to save the variables\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers,\n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        \n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss,\n",
    "                                                 model.final_state,\n",
    "                                                 model.optimizer],\n",
    "                                                 feed_dict = feed)\n",
    "            \n",
    "            \n",
    "            end = time.time()\n",
    "            #control the print lines\n",
    "            # for each 100 times\n",
    "            if counter % 100 == 0:\n",
    "                print('round: {}/{}...'.format(e+1, epochs),\n",
    "                      'training steps (batch): {}...'.format(counter),\n",
    "                      'training error: {:.4f}...'.format(batch_loss),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "                \n",
    "                if (counter % save_every_n == 0):\n",
    "                    saver.save(sess, 'checkpoints/i{}_1{}.ckpt'.format(counter, lsmt_size))\n",
    "                    \n",
    "    saver.save(sess, 'checkpoints/i{}_l{}.ckpt'.format(counter, lstm_size))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
