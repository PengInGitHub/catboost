{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "# two models: Skip-Gram and CBow\n",
    "# CBOW: predict input word based on context\n",
    "# Skip-Gram: predict context based on input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two steps:\n",
    "# 1. Build model - Fake task - auto encoder\n",
    "# 2. Get embedding vector from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ptep 1: Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: the dog barked at the mailman\n",
    "# input word\n",
    "# skip_window -> span = skip_window * 2\n",
    "# num_skips: (input word, output word) * num_skips\n",
    "# output of the model: probability of words to be collocate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: The quick brown fox jumps over lazy dog\n",
    "# skip_window = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do we represent these words?\n",
    "# train text -> vocabulary -> one hot encoding\n",
    "# hyper-parameters: dimensions of word vector; window_size\n",
    "# hidden layer: each row is the word vector of each word\n",
    "\n",
    "# hidden layer\n",
    "# sparse matrix computation\n",
    "# hidden layer weight matrix -> lookup table\n",
    "# no need to compute, just look up values in the weight matrix\n",
    "# where the corresponding input vector is not zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output layer\n",
    "# softmax classifier, output probability\n",
    "\n",
    "# example 'ants'\n",
    "# word vector of ants (1*300, 300 is the num of nodes in hidden layer)\n",
    "# softmax: (e**x)/(sum(e**x))\n",
    "\n",
    "# analysis: in case two words share similar context aka similar window words\n",
    "# Kitty climmed the tree.; Cat climmed the tree.\n",
    "\n",
    "# stemming: ant - ants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Training Skip_Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec is super big\n",
    "# Doing Gradient Descent in such a huge NN is time-consuming\n",
    "# It needs tons of data otherwise it may be overfitted\n",
    "\n",
    "# word pairs\n",
    "# sample highly frequent words to reduce the num of training samples\n",
    "# negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Word Pairs and Pharses\n",
    "# 2.2 Sampling on high-frequency vocabulary\n",
    "# Sampling rate\n",
    "# 2.3 Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# Build input for training\n",
    "# Build model\n",
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarchism originated\n"
     ]
    }
   ],
   "source": [
    "with open ('/Users/pliu/Downloads/text8.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "print(text[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "# remove non-char and low-frequency letters\n",
    "# word-split from text\n",
    "# construct input\n",
    "# mapping of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, freq=5):\n",
    "    \"\"\"\n",
    "    process input data\n",
    "    args:\n",
    "        text: the text to process\n",
    "        freq: lower threshold for frequency of a word\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace('.','<PERIOD> ')\n",
    "    text = text.replace(',','<COMMA> ')\n",
    "    text = text.replace('\"','<QUOTATION_MARK> ')\n",
    "    text = text.replace(';','<SEMICOLON> ')\n",
    "    text = text.replace('!','<EXCLAMATION_MARK> ')\n",
    "    text = text.replace('?','<QUESTION_MARK> ')\n",
    "    text = text.replace('(','<LEFT_PAREN> ')\n",
    "    text = text.replace(')','<RIGHT_PAREN> ')\n",
    "    text = text.replace('--','<HYPHENS> ')\n",
    "    text = text.replace(':','<COLON> ')\n",
    "    words = text.split()\n",
    "\n",
    "    # remove low-frequency words to reduce noise\n",
    "    word_counts = Counter(words)\n",
    "    trimmed_words = [word for word in words if word_counts[word] > freq]\n",
    "    \n",
    "    return trimmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarchism originated as a term\n",
      "['anarchism', 'as', 'a', 'of', 'first', 'the', 'of', 'the', 'and', 'the', 'of', 'the', 'the', 'is', 'in', 'a', 'to', 'that', 'to', 'the', 'of', 'society', 'it', 'as', 'a', 'by', 'anarchists', 'the', 'anarchism', 'is']\n"
     ]
    }
   ],
   "source": [
    "print(text[:30])\n",
    "words = preprocess(text)\n",
    "print(words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the mapping \n",
    "vocab = set(words)\n",
    "vocab_to_int = {w:c for c,w in enumerate(vocab)}\n",
    "int_to_vocab = {c:w for c,w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "print(len(words))\n",
    "print(len(set(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping from vocab to int\n",
    "int_words = [vocab_to_int[w] for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({15: 78, 21: 55, 7: 40, 3: 37, 32: 24, 16: 20, 23: 20, 27: 19, 31: 18, 6: 15, 33: 14, 19: 13, 10: 13, 12: 13, 25: 13, 34: 10, 18: 10, 2: 9, 13: 9, 24: 9, 0: 9, 26: 9, 14: 8, 20: 8, 4: 8, 5: 8, 30: 8, 8: 7, 17: 7, 11: 7, 29: 7, 1: 6, 9: 6, 22: 6, 28: 6})\n"
     ]
    }
   ],
   "source": [
    "int_word_counts = Counter(int_words)\n",
    "\n",
    "print(int_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling\n",
    "t = 1e-3\n",
    "threshold = 0.8\n",
    "\n",
    "int_word_counts = Counter(int_words)\n",
    "\n",
    "# calculate frequency of each word\n",
    "# total\n",
    "total_count = len(int_words)\n",
    "# for each\n",
    "word_freq = {w: c/total_count for w, c in int_word_counts.items()}\n",
    "# compute probability to be deleted\n",
    "prob_drop = {w:1-np.sqrt(t/word_freq[w]) for w in int_word_counts}\n",
    "# sampling the words\n",
    "train_words = [w for w in int_words if prob_drop[w] < threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch\n",
    "# Skip-Gram: predict context by input words\n",
    "def get_targets(words, idx, window_size):\n",
    "    \"\"\"\n",
    "    get context word list of an input word\n",
    "    args:\n",
    "        words: vocab list\n",
    "        idx: index of input word\n",
    "        window_size: the size of sliding window\n",
    "    return:\n",
    "        list of context words of an input word\n",
    "    \"\"\"\n",
    "    target_window = np.random.randint(1, window_size+1)\n",
    "    # compute index of start and end point of the sliding window\n",
    "    # take into account the situation that input word may have no sufficient words in front\n",
    "    start_point = idx - target_window if (idx - target_window) > 0 else 0\n",
    "    end_point = idx + target_window\n",
    "    \n",
    "    # output words, the context words in window\n",
    "    targets = words[start_point:idx] + words[idx+1:end_point+1]\n",
    "    targets = set(targets)\n",
    "    return list(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(words, batch_size, window_size):\n",
    "    \"\"\"\n",
    "    a batch generator\n",
    "    args:\n",
    "        words: training words\n",
    "        batch_size: num of words in each batch\n",
    "        window_size: sliding window size\n",
    "    \"\"\"\n",
    "    n_batches = len(words) // batch_size\n",
    "    \n",
    "    # full batches only\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    # get batch\n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx: idx+batch_size]\n",
    "        \n",
    "        for i in range(len(batch)):\n",
    "            batch_x = batch[i]\n",
    "            batch_y = get_targets(batch, i, word_size)\n",
    "            \n",
    "            x.extend([batch_x*len(batch_y)])\n",
    "            y.extend(batch_y)\n",
    "            \n",
    "        yield x, y         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build NN\n",
    "\n",
    "# input layer\n",
    "# embedding layer\n",
    "# negative sampling (accelerating BP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    inputs = tf.placeholder(tf.int32, shape=[None], name='inputs')\n",
    "    labels = tf.placeholder(tf.int32, shape=[None,None], name='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding layer\n",
    "# embedding matrix: vocab_size*hidden_units_size\n",
    "# embedding matrix as lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(int_to_vocab)\n",
    "embedding_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct embedding matrix\n",
    "with train_graph.as_default():\n",
    "    # embedding weight matrix\n",
    "    embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, -1))\n",
    "    # lookup\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Sampling\n",
    "# to accelerate Gradient Descent\n",
    "n_sampled = 100\n",
    "with train_graph.as_default():\n",
    "    softmax_w = tf.Variable(tf.truncated_normal([vocab_size, embedding_size], stddev=0.1))\n",
    "    softmax_b = tf.Variable(tf.zeros(vocab_size))\n",
    "\n",
    "    # calculate loss under negative sampling\n",
    "    loss = tf.nn.sampled_softmax_loss(softmax_w, softmax_b, labels, embed, n_sampled, vocab_size)\n",
    "    \n",
    "    cost = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "# check similar sentences of inputs\n",
    "with train_graph.as_default():\n",
    "    # pick up words randomly\n",
    "    valid_size = 16\n",
    "    valid_window = 100\n",
    "    \n",
    "    # choose words for valudation from differennt positions\n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples, \n",
    "                               random.sample(range(1000, 1000+valid_window), valid_size//2))\n",
    "    \n",
    "    valid_size = len(valid_examples)\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # standardize each embedding ()\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keepdims=True))\n",
    "    normalized_embedding = embedding / norm\n",
    "    \n",
    "    # lookup embedding of word for validation\n",
    "    valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)\n",
    "    \n",
    "    # compute cosine similarity\n",
    "    similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10 # umber of iteration\n",
    "batch_size = 1000 # size of batch\n",
    "window_size = 10 # size of sliding window\n",
    "\n",
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # do iteration for epochs times\n",
    "    for e in in range(1, epochs+1):\n",
    "        batches = get_batches(train_words, batch_size, window_size)\n",
    "        start = time.time()\n",
    "        \n",
    "        for x, y in batches:\n",
    "            feed = {inputs:x,\n",
    "                    labels:np.array(y)[:,None]}\n",
    "            train_loss, _ = sess.run([cost, optimizer], feed_dict = feed)\n",
    "            \n",
    "            loss += train_loss\n",
    "            \n",
    "            # for each 100 times\n",
    "            if iteration % 100 == 0:\n",
    "                end = time.time()\n",
    "                # log training \n",
    "                print('Epoch{}/{}'.format(e, epochs),\n",
    "                      'Iteration: {}'.format(iteration),\n",
    "                      'Avg Training loss:{:.4f}'.format(loss/100),\n",
    "                      '{:.4f} sec/batch'.format((end-start)/100))\n",
    "                \n",
    "                \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
